{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c74e9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import models, transforms\n",
    "from torch import nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.random_projection import SparseRandomProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4d511e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Settings ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRAIN_DIR = \"masks/good\"\n",
    "TEST_DIR_GOOD = \"masks/test/good\"\n",
    "TEST_DIR_DEFECT = \"masks/test/defective\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f659c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DDP Engineering\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DDP Engineering\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet50_2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# --- Model Setup ---\n",
    "resnet = models.wide_resnet50_2(pretrained=True).to(DEVICE)\n",
    "resnet.eval()\n",
    "\n",
    "features = {}\n",
    "\n",
    "def save_hook(name):\n",
    "    def fn(_, __, output): features[name] = output\n",
    "    return fn\n",
    "\n",
    "resnet.layer2[-1].register_forward_hook(save_hook(\"layer2\"))\n",
    "resnet.layer3[-1].register_forward_hook(save_hook(\"layer3\"))\n",
    "\n",
    "# --- Image Transform ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd17bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Extraction ---\n",
    "def extract_patch_features(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    x = transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = resnet(x)\n",
    "        f1 = nn.functional.avg_pool2d(features['layer2'], 3, 1, 1)\n",
    "        f2 = nn.functional.avg_pool2d(features['layer3'], 3, 1, 1)\n",
    "        f2 = nn.functional.interpolate(f2, size=f1.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        emb = torch.cat([f1, f2], dim=1)\n",
    "        emb = emb.permute(0, 2, 3, 1).reshape(-1, emb.shape[1])\n",
    "        return emb.cpu().numpy()  # [N_patches, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88a67abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting features from training (good) images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:01<00:00, 19.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Build Memory Bank from Good Training Images ---\n",
    "print(\"üîç Extracting features from training (good) images...\")\n",
    "all_patches = []\n",
    "for fname in tqdm(os.listdir(TRAIN_DIR)):\n",
    "    if fname.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        path = os.path.join(TRAIN_DIR, fname)\n",
    "        all_patches.append(extract_patch_features(path))\n",
    "all_patches = np.vstack(all_patches)  # [N, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7fe4105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Building coreset (memory bank)...\n"
     ]
    }
   ],
   "source": [
    "# --- Coreset Sampling ---\n",
    "print(\"üéØ Building coreset (memory bank)...\")\n",
    "projector = SparseRandomProjection(eps=0.5)\n",
    "projected = projector.fit_transform(all_patches)\n",
    "mem_bank = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42e056a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [00:07<00:00, 31.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# K-Center Greedy\n",
    "n_keep = int(len(projected) * 0.01)  # 1% of patches\n",
    "selected = []\n",
    "distances = None\n",
    "for _ in tqdm(range(n_keep)):\n",
    "    if distances is None:\n",
    "        idx = np.random.randint(0, len(projected))\n",
    "        centroid = projected[idx:idx+1]\n",
    "        distances = pairwise_distances(projected, centroid)\n",
    "        selected.append(idx)\n",
    "    else:\n",
    "        idx = np.argmax(distances)\n",
    "        centroid = projected[idx:idx+1]\n",
    "        new_dist = pairwise_distances(projected, centroid)\n",
    "        distances = np.minimum(distances, new_dist)\n",
    "        selected.append(idx)\n",
    "memory_bank = torch.tensor(all_patches[selected]).float().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83b43674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Anomaly Scoring ---\n",
    "def anomaly_score(img_path):\n",
    "    patch_feats = extract_patch_features(img_path)\n",
    "    with torch.no_grad():\n",
    "        dists = torch.cdist(torch.tensor(patch_feats).float().to(DEVICE), memory_bank)\n",
    "        min_dists = dists.min(dim=1).values  # distance to closest memory patch\n",
    "    return min_dists.max().item()  # worst-case patch distance\n",
    "\n",
    "# --- Scoring Function for a Folder ---\n",
    "def score_folder(folder):\n",
    "    scores = []\n",
    "    for fname in tqdm(os.listdir(folder)):\n",
    "        if fname.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            path = os.path.join(folder, fname)\n",
    "            score = anomaly_score(path)\n",
    "            scores.append((fname, score))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3091821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DDP Engineering\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DDP Engineering\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet50_2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Feature extractor: ResNet50 up to layer2 & layer3\n",
    "res = models.wide_resnet50_2(pretrained=True).to(device)\n",
    "layers = ['layer2', 'layer3']\n",
    "features = {}\n",
    "def hook(name):\n",
    "    def fn(m, i, o): features[name] = o\n",
    "    return fn\n",
    "for name in layers:\n",
    "    getattr(res, name)[-1].register_forward_hook(hook(name))\n",
    "res.eval()\n",
    "\n",
    "# Image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36a77fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Extract patch-features from good images ---\n",
    "def extract_patches(img):\n",
    "    img = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = res(x)\n",
    "        f1 = nn.functional.avg_pool2d(features['layer2'], 3, 1, 1)\n",
    "        f2 = nn.functional.avg_pool2d(features['layer3'], 3, 1, 1)\n",
    "    f2 = nn.functional.interpolate(f2, size=f1.shape[-2:])\n",
    "    emb = torch.cat([f1, f2], dim=1)  # b√óC√óH√óW\n",
    "    emb = emb.permute(0,2,3,1).reshape(-1, emb.shape[1])\n",
    "    return emb.cpu().numpy()\n",
    "\n",
    "good = \"good/\"\n",
    "all_patches = []\n",
    "for fn in os.listdir(good):\n",
    "    if fn.lower().endswith(('.jpg','.png','jpeg')):\n",
    "        all_patches.append(extract_patches(os.path.join(good, fn)))\n",
    "all_patches = np.vstack(all_patches)  # [N, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc83a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Coreset subsampling via k-Center Greedy ---\n",
    "rp = SparseRandomProjection(eps=0.5)\n",
    "proj = rp.fit_transform(all_patches)\n",
    "selected = []\n",
    "dist = None\n",
    "total = proj.shape[0]\n",
    "sub = int(total * 0.01)  # keep 1%\n",
    "for _ in range(sub):\n",
    "    if dist is None:\n",
    "        idx = np.random.randint(0, total)\n",
    "        cent = proj[idx:idx+1]\n",
    "        dist = pairwise_distances(proj, cent).reshape(-1,1)\n",
    "        selected = [idx]\n",
    "    else:\n",
    "        idx = np.argmax(dist)\n",
    "        cent = proj[idx:idx+1]\n",
    "        newd = pairwise_distances(proj, cent).reshape(-1,1)\n",
    "        dist = np.minimum(dist, newd)\n",
    "        selected.append(idx)\n",
    "memory = torch.from_numpy(all_patches[selected]).float().to(device)  # memory bank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4afe53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Test images ---\n",
    "def anomaly_score(img_path):\n",
    "    patches = extract_patches(img_path)\n",
    "    with torch.no_grad():\n",
    "        d = torch.cdist(torch.from_numpy(patches).float().to(device), memory)\n",
    "        dmin = d.min(dim=1).values\n",
    "    return dmin.max().item()  # take worst patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e7ef2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Scoring GOOD test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Scoring DEFECTIVE test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Score Good and Defective Images ---\n",
    "print(\"üìä Scoring GOOD test images...\")\n",
    "scores_good = score_folder(TEST_DIR_GOOD)\n",
    "\n",
    "print(\"üìä Scoring DEFECTIVE test images...\")\n",
    "scores_defect = score_folder(TEST_DIR_DEFECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d781f98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Threshold set at 95th percentile of GOOD scores: 2.1202\n",
      "\n",
      "--- Results ---\n",
      "GOOD | Image__2025-05-05__10-06-44.jpg | Score: 2.1202 | Normal ‚úÖ\n",
      "DEFECT | 33af61dd-Image__2025-05-02__10-36-24.jpg | Score: 2.5506 | ANOMALY üö®\n"
     ]
    }
   ],
   "source": [
    "# --- Threshold Selection ---\n",
    "all_good_values = [s for _, s in scores_good]\n",
    "threshold = np.percentile(all_good_values, 95)\n",
    "print(f\"‚úÖ Threshold set at 95th percentile of GOOD scores: {threshold:.4f}\")\n",
    "\n",
    "# --- Final Decision and Output ---\n",
    "print(\"\\n--- Results ---\")\n",
    "def print_results(scores, label):\n",
    "    for fname, score in scores:\n",
    "        result = \"ANOMALY üö®\" if score > threshold else \"Normal ‚úÖ\"\n",
    "        print(f\"{label} | {fname:<30} | Score: {score:.4f} | {result}\")\n",
    "\n",
    "print_results(scores_good, \"GOOD\")\n",
    "print_results(scores_defect, \"DEFECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58c9f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def auto_crop_mask(img_path, output_crop_path=None, output_mask_path=None):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define white-ish color range in HSV (tweak as needed)\n",
    "    lower_white = np.array([0, 0, 200])\n",
    "    upper_white = np.array([180, 40, 255])\n",
    "\n",
    "    mask = cv2.inRange(img_hsv, lower_white, upper_white)\n",
    "\n",
    "    # Morphological cleanup\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15,15))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Find contours on the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        print(\"No white part detected.\")\n",
    "        return None, None\n",
    "\n",
    "    # Find largest contour (assuming it‚Äôs your part)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Get bounding box around largest contour\n",
    "    x,y,w,h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "    # Crop original image to bounding box\n",
    "    cropped = img[y:y+h, x:x+w]\n",
    "\n",
    "    # Crop mask to same bounding box\n",
    "    mask_cropped = mask[y:y+h, x:x+w]\n",
    "\n",
    "    # Save if paths provided\n",
    "    if output_crop_path:\n",
    "        cv2.imwrite(output_crop_path, cropped)\n",
    "    if output_mask_path:\n",
    "        cv2.imwrite(output_mask_path, mask_cropped)\n",
    "\n",
    "    return cropped, mask_cropped\n",
    "\n",
    "# Example batch processing\n",
    "input_dir = \"test\\defective\"\n",
    "crop_dir = \"cropped/\"\n",
    "mask_dir = \"masks/\"\n",
    "os.makedirs(crop_dir, exist_ok=True)\n",
    "os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "for fname in os.listdir(input_dir):\n",
    "    if fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "        img_path = os.path.join(input_dir, fname)\n",
    "        crop_path = os.path.join(crop_dir, fname)\n",
    "        mask_path = os.path.join(mask_dir, fname)\n",
    "        cropped_img, mask_img = auto_crop_mask(img_path, crop_path, mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d7df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
